{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e16c49",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "----------------------------\n",
    "\n",
    "# Sepideh Khalafi\n",
    "------------------------------\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "This is a labeled sentiment analysis dataset on tweets from different topics on Twitter. ([Kaggel](https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets), 2022) The text is labeled for four different categories.  \n",
    "\n",
    "---------------------------------\n",
    "\n",
    "## The Problem\n",
    "This task is a supervised sentiment analysis on tweets. We will try and build an NLP pipeline to classify the sentiment of a given text and review. The target language would be **English**. We need to extract the english teewts and their lable to build our classification model on predincting sentiment of a given tweet. \n",
    "\n",
    "\n",
    "----------------\n",
    "## Importing Libraries\n",
    "\n",
    "* *pandas* is for table processing\n",
    "* *nltk* and *re* have been used for processing the text\n",
    "* *numpy* is used for hadling array form data\n",
    "* *sklearn.model_selection* is to split the data\n",
    "* *imblearn_undersampling* is for resampling the data\n",
    "* *sklearn.preprocessing* is used for encoding target labels.\n",
    "* *sklearn_feature_extraction* has been used for vectorisatoin\n",
    "* *sklearn.naive_bayes* and *sklearn.ensemble* are used for classification.\n",
    "* *gensim* is used to download the pretrained model for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6512fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "import imblearn.under_sampling\n",
    "\n",
    "import sklearn.feature_extraction.text\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b64ad",
   "metadata": {},
   "source": [
    "## Data Acquisition and Prprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67b2d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937854, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Charlie_Corley @Kristine1G @amyklobuchar @Sty...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BadBunny: Como dos gotas de agua: Joven se di...</td>\n",
       "      <td>es</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://t.co/YJNiO0p1JV Flagstar Bank disclose...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rwanda is set to host the headquarters of Unit...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OOPS. I typed her name incorrectly (today’s br...</td>\n",
       "      <td>en</td>\n",
       "      <td>litigious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language      Label\n",
       "0  @Charlie_Corley @Kristine1G @amyklobuchar @Sty...       en  litigious\n",
       "1  #BadBunny: Como dos gotas de agua: Joven se di...       es   negative\n",
       "2  https://t.co/YJNiO0p1JV Flagstar Bank disclose...       en  litigious\n",
       "3  Rwanda is set to host the headquarters of Unit...       en   positive\n",
       "4  OOPS. I typed her name incorrectly (today’s br...       en  litigious"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pandas.read_csv(\"../datasets/my_datasets/dataset.csv\")\n",
    "\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1634e1",
   "metadata": {},
   "source": [
    "This is a large dataset of 937854 texts from Twitter with only 3 features: *Text* which we will work on, *Language* indicating the language of the text, and *label* indicating the sentiment that is our target label.\n",
    "\n",
    "The target is of four different classes:\n",
    "- neagative\n",
    "- positive\n",
    "- litigious\n",
    "- uncertainty\n",
    "\n",
    "Using *groupby* on *language* column, we can find out how many tweets there are for each language after *counting* them. It is sorted alphabetically by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdc7cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[Photo(previewUrl='https://pbs.twimg.com/media/EPYG2rKVAAA1e_O?format=jpg&amp;name=small', fullUrl='https://pbs.twimg.com/media/EPYG2rKVAAA1e_O?format=jpg&amp;name=large')]</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[Photo(previewUrl='https://pbs.twimg.com/media/FWV94O7UEAAjMOi?format=jpg&amp;name=small', fullUrl='https://pbs.twimg.com/media/FWV94O7UEAAjMOi?format=jpg&amp;name=large')]</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[Photo(previewUrl='https://pbs.twimg.com/media/FWVRZ6bWYAAM8-3?format=jpg&amp;name=small', fullUrl='https://pbs.twimg.com/media/FWVRZ6bWYAAM8-3?format=jpg&amp;name=large')]</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[Photo(previewUrl='https://pbs.twimg.com/media/FWVhrnXX0AE0h9Z?format=jpg&amp;name=small', fullUrl='https://pbs.twimg.com/media/FWVhrnXX0AE0h9Z?format=jpg&amp;name=large')]</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[Photo(previewUrl='https://pbs.twimg.com/media/FWWBzWTXkAAyZqm?format=jpg&amp;name=small', fullUrl='https://pbs.twimg.com/media/FWWBzWTXkAAyZqm?format=jpg&amp;name=large')]</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>und</th>\n",
       "      <td>2702</td>\n",
       "      <td>2702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vi</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zxx</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Label\n",
       "Language                                                       \n",
       "[Photo(previewUrl='https://pbs.twimg.com/media/...     1      1\n",
       "[Photo(previewUrl='https://pbs.twimg.com/media/...     1      1\n",
       "[Photo(previewUrl='https://pbs.twimg.com/media/...     1      1\n",
       "[Photo(previewUrl='https://pbs.twimg.com/media/...     1      1\n",
       "[Photo(previewUrl='https://pbs.twimg.com/media/...     1      1\n",
       "...                                                  ...    ...\n",
       "und                                                 2702   2702\n",
       "ur                                                    42     42\n",
       "vi                                                    32     32\n",
       "zh                                                   248    248\n",
       "zxx                                                   85     85\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.groupby(by = \"Language\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bb8a8",
   "metadata": {},
   "source": [
    "There are 72 different values for languages and some are not valid languages. We need to process on *English* tweets. We can find about the number of the english tweets by counting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e274d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        871310\n",
       "Language    871310\n",
       "Label       871310\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[tweets[\"Language\"] == \"en\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbbeeb",
   "metadata": {},
   "source": [
    "Out of the whole 937854 tweets, 871310 of them are in *English*. And after finding the english tweets, we don't need the column. So we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b304d1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871310, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_en = tweets.loc[tweets[\"Language\"] == \"en\"]\n",
    "tweets_en = tweets_en.drop([\"Language\"], axis = 1)\n",
    "tweets_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12ee2c",
   "metadata": {},
   "source": [
    "Since this is a large dataset and the hardware and software is not enough to process all of it, we take a sample of 5000. The data is texual and the task is time consuming. (even with this size the gradient Bossting model for training takes almost an hour to work and give a result)\n",
    "\n",
    "We then define the *X* and *Y* which are both only one column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd715556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train:  (3750,)\n",
      "shape of test:  (1250,)\n"
     ]
    }
   ],
   "source": [
    "tweets_en = tweets_en.sample(n = 5000)\n",
    "\n",
    "tweets_x = tweets_en[\"Text\"]\n",
    "tweets_y = tweets_en[\"Label\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = sklearn.model_selection.train_test_split(tweets_x, tweets_y)\n",
    "\n",
    "print(\"shape of train: \", train_x.shape)\n",
    "print(\"shape of test: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea784f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_x.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b315dc",
   "metadata": {},
   "source": [
    "There are no *Null* values in the dataset.\n",
    "\n",
    "Let's discover the proportion and labels of the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc24422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative       1063\n",
       "positive       1060\n",
       "uncertainty     865\n",
       "litigious       762\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb2502",
   "metadata": {},
   "source": [
    "The proportion of the target label is not too imbalanced. But it would be better to have a balanced one. I took a large enough sample from the original dataset, so that I can do undersampling here with enough data points to avoid dealing with artificially generated data.\n",
    "\n",
    "I needed to reshape the train set because of an error that was because of the 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408aea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train_x after undersampling is: (3048, 1)\n",
      "the shape of train_y after undersampling is: (3048,)\n"
     ]
    }
   ],
   "source": [
    "under_sampling = imblearn.under_sampling.RandomUnderSampler()\n",
    "under_x, under_y = under_sampling.fit_resample(train_x.values.reshape(-1,1), train_y)\n",
    "\n",
    "print(f\"the shape of train_x after undersampling is: {under_x.shape}\")\n",
    "print(f\"the shape of train_y after undersampling is: {under_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a22596",
   "metadata": {},
   "source": [
    "The type of the undersample data is *numpy.ndarray* and we can't apply the preprocessing on this type. It needs to be converted to series object and to be able to do that we have to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03368e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of the train set is: <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3048,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = pandas.Series(under_x.reshape(-1))\n",
    "print(f\"type of the train set is: {type(train_x)}\")\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f2f1ce",
   "metadata": {},
   "source": [
    "Here we define a function to clean and process the text for vectorization.\n",
    "\n",
    "* First, we just lower all the letters.\n",
    "* Using *re* library and *sub* function I remove numbrs and digits, since they do not express sentiment.\n",
    "* Then using *NLTK* library and *RegexpTokenizer* class we find all the tokens. The speicial charechters and spaces that I have defined are going to be eliminated.\n",
    "* Next, we find each word's stem, meaning eliminating the prefix or sufix\n",
    "* finally, we join these rooted words and join them to have the text back and return it\n",
    "\n",
    "I defined a small corpus for **stopwords**. The one that belongs to *nltk* has many stemmed and lemmatized words that are about sentiment, like negative verbs. So I just decided to define a shorter one containing pronouns without verbs, based on nltk's corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1096f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i',\n",
    "             'me',\n",
    "             'my',\n",
    "             'myself',\n",
    "             'we',\n",
    "             'our',\n",
    "             'ours',\n",
    "             'ourselves',\n",
    "             'you',\n",
    "             'your',\n",
    "             'yours',\n",
    "             'yourself',\n",
    "             'yourselves',\n",
    "             'he',\n",
    "             'him',\n",
    "             'his',\n",
    "             'himself',\n",
    "             'she',\n",
    "             'her',\n",
    "             'hers',\n",
    "             'herself',\n",
    "             'it',\n",
    "             'its',\n",
    "             'itself',\n",
    "             'they',\n",
    "             'them',\n",
    "             'their',\n",
    "             'theirs',\n",
    "             'themselves',\n",
    "             'what',\n",
    "             'which',\n",
    "             'who',\n",
    "             'whom',\n",
    "             'this',\n",
    "             'that',\n",
    "             'these',\n",
    "             'those'\n",
    "            ]\n",
    "\n",
    "\n",
    "def tweet_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+[^@*!#$.%&? (0-9)]*\")\n",
    "    words = tokenizer.tokenize(text)\n",
    "    snowball = nltk.stem.snowball.SnowballStemmer(language = \"english\")\n",
    "    stemmed_words = [snowball.stem(w) for w in words if w not in stopwords]\n",
    "    processed_text = \" \".join(stemmed_words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e430b147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3048,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.apply(tweet_preprocess)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb70c5",
   "metadata": {},
   "source": [
    "Since we would have to do this part for the test set anyway, we can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd7ae8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = test_x.apply(tweet_preprocess)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d4263",
   "metadata": {},
   "source": [
    "## Feature Engineering, and Text Representation\n",
    "\n",
    "\n",
    "\n",
    "We need to represent our text with vectors to be able to process it by the model.\n",
    "\n",
    "### TF-IDF Vectorizer\n",
    "\n",
    "TF-IDF returns a matrix, so we convert it to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7573b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3048, 13438)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "vectorize_x = vectorizer.fit_transform(train_x).toarray()\n",
    "print(vectorize_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa059c9",
   "metadata": {},
   "source": [
    "### Word2Vec Vectors\n",
    "\n",
    "I chose a pretrained model from *hugging face* that is trained on Tweets: \"Pre-trained glove vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased.\" ([hugging face](https://huggingface.co/fse/glove-wiki-gigaword-300), 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f9f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_pretrained = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60976f2a",
   "metadata": {},
   "source": [
    "Word2vec models can not be used on sentences or phrases. We need to apply it on each word and calculate the average of them for each text. I defined a function for this purpose. (We have already processed the text. This is just for vectorizing.)\n",
    "\n",
    "First we need to define a casual tokenizer to handle the text in the training set in our function. This part is out of the function. \n",
    "\n",
    "In the function:\n",
    "- First we tokenize the text\n",
    "- then we create a vector of size 300 (that should be enough) and it should be zero. We will add each word's weight, defined by the word2vec object, to this vector in the for loop.\n",
    "- we define a counter for the words and in the end we calculate the average weight of the vector with it.\n",
    "- the numbers can be negative and classifier could not deal with it, so I added aconstant number of 10 (large enough) to make it a positive value.\n",
    "- this function returns a vwctor with average weight of the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e51cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "def w2v_vectorizer(text):\n",
    "    words = tokenizer.tokenize(text)\n",
    "    vector = numpy.zeros(300)\n",
    "    word_numbers = 0\n",
    "    for w in words:\n",
    "        if w in word2vec_pretrained:\n",
    "            vector += word2vec_pretrained[w]\n",
    "            word_numbers += 1\n",
    "    if word_numbers > 0:\n",
    "        vector /= word_numbers\n",
    "        vector += 10\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73124ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train set after applying word2vec vectorization: (3048,)\n",
      "the length of each data point: 300\n"
     ]
    }
   ],
   "source": [
    "w2v = train_x.apply(w2v_vectorizer)\n",
    "print(f\"the shape of the train set after applying word2vec vectorization: {w2v.shape}\")\n",
    "print(f\"the length of each data point: {len(w2v[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553659e2",
   "metadata": {},
   "source": [
    "The outcome is a series object with only one feature. And this feature is an array of size 300. sklearn models cannot deal with it and it geneates and error while using this dataset. So I have to put every value of the arrays in one feature of a dataframe. I defined a zero matrix of rows equal to the trainig dataset and features of 300. while iterating over the rows of the dataset, I added each value of that to the matrix. In the end I converted the matrix to a dataframe.\n",
    "\n",
    "It could not be done in the first place by just converting the output to a dataframe. Each value should have been extracted from datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da5a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = numpy.zeros((len(w2v.axes[0]), 300))\n",
    "\n",
    "for i in range(len(w2v.axes[0])):\n",
    "    matrix[i] = matrix[i] + w2v[i]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07d33ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_train_x = pandas.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4dfda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3048, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf39e4",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "For modeling we try a simple classifier, Naive Bayes, and a transformer, from a simple transformer library, to see which one has a better performance on our modeling.\n",
    "\n",
    "\n",
    "### Naive Bayes and TF-IDF\n",
    "\n",
    "Multinomial Naive Bayes classifier does not have many features. I tried a few values for *alpha* parameter. \n",
    "\n",
    "Here we pass the dataset vectorized by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de2a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer:\n",
      "best accuracy of 'naive bayes' model with Tf_IDf is 0.86024011413497\n",
      "best parameters for 'naive bayes' model with Tf_IDf is{'alpha': 4.0}\n"
     ]
    }
   ],
   "source": [
    "naive_parameters = {\"alpha\" : [1.7, 1.8, 1.9, 2.0, 2.5, 3.0, 3.5, 4.0]}\n",
    "naive_classifier = sklearn.naive_bayes.MultinomialNB()\n",
    "naive_classifier_model_tf = sklearn.model_selection.GridSearchCV(naive_classifier, \n",
    "                                                              naive_parameters, \n",
    "                                                              scoring = \"accuracy\",\n",
    "                                                              cv = 5,\n",
    "                                                              n_jobs = -1\n",
    "                                                             )\n",
    "naive_classifier_model_tf.fit(vectorize_x, under_y)\n",
    "\n",
    "print(\"TF-IDF vectorizer:\")\n",
    "print(f\"best accuracy of 'naive bayes' model with Tf_IDf is {naive_classifier_model_tf.best_score_}\")\n",
    "print(f\"best parameters for 'naive bayes' model with Tf_IDf is{naive_classifier_model_tf.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d72f2",
   "metadata": {},
   "source": [
    "With rasing *alpha* no significant improvement was shown.\n",
    "\n",
    "### Naive Bayes and word2vec\n",
    "\n",
    "Here I do the same only this time the vectorizer is the function I defined using word2vec predefiend model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d433010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec vectorizer:\n",
      "best accuracy of 'naive bayes' model with word2vec is 0.4849056502193868\n",
      "best parameters for 'naive bayes' model with word2vec is{'alpha': 0.2}\n"
     ]
    }
   ],
   "source": [
    "naive_parameters = {\"alpha\" : [0.2, 0.3, 0.4, 0.5]}\n",
    "naive_classifier = sklearn.naive_bayes.MultinomialNB()\n",
    "naive_classifier_model_w2v = sklearn.model_selection.GridSearchCV(naive_classifier, \n",
    "                                                                  naive_parameters, \n",
    "                                                                  scoring = \"accuracy\",\n",
    "                                                                  cv = 5,\n",
    "                                                                  n_jobs = -1\n",
    "                                                                 )\n",
    "\n",
    "\n",
    "naive_classifier_model_w2v.fit(w2v_train_x, under_y)\n",
    "\n",
    "print(\"word2vec vectorizer:\")\n",
    "print(f\"best accuracy of 'naive bayes' model with word2vec is {naive_classifier_model_w2v.best_score_}\")\n",
    "print(f\"best parameters for 'naive bayes' model with word2vec is{naive_classifier_model_w2v.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9749b06",
   "metadata": {},
   "source": [
    "### Gradient Boosting and TF-IDF\n",
    "\n",
    "For this classifier model I tried to find best values for these parameter: *learning rate* which is for the contribution of the trees, *criterion* that defines the measurement of the quality of splitting, and *max_depth* that is for the maximum depth of each node. (It takes almost an hour to run and get results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4e6e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer:\n",
      "best accuracy of 'gradient boosting' model with Tf_IDf is 0.9609588414223801\n",
      "best parameters for 'gradient boosting' model with Tf_IDf is{'criterion': 'friedman_mse', 'learning_rate': 0.5, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "gboosting_parameters = {\"learning_rate\" : [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                        \"criterion\" : [\"friedman_mse\", \"squared_error\"],\n",
    "                        \"max_depth\" : (2, 5)\n",
    "                       }\n",
    "gboosting = sklearn.ensemble.GradientBoostingClassifier()\n",
    "gboosting_classifier_tf = sklearn.model_selection.GridSearchCV(gboosting, \n",
    "                                                            gboosting_parameters,\n",
    "                                                            scoring = \"accuracy\",\n",
    "                                                            cv = 5,\n",
    "                                                            n_jobs = -1\n",
    "                                                           )\n",
    "\n",
    "gboosting_classifier_tf.fit(vectorize_x, under_y)\n",
    "\n",
    "print(\"TF-IDF vectorizer:\")\n",
    "print(f\"best accuracy of 'gradient boosting' model with Tf_IDf is {gboosting_classifier_tf.best_score_}\")\n",
    "print(f\"best parameters for 'gradient boosting' model with Tf_IDf is{gboosting_classifier_tf.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ba6e0",
   "metadata": {},
   "source": [
    "### Gradient Boosting and Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cdf1278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec vectorizer:\n",
      "best accuracy of 'gradient boosting' model with word2vec is 0.6407580284799053\n",
      "best parameters for 'gradient boosting' model with word2vec is{'criterion': 'friedman_mse', 'learning_rate': 0.5, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "gboosting_parameters = {\"learning_rate\" : [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                        \"criterion\" : [\"friedman_mse\", \"squared_error\"],\n",
    "                        \"max_depth\" : (2, 5)\n",
    "                       }\n",
    "gboosting = sklearn.ensemble.GradientBoostingClassifier()\n",
    "gboosting_classifier_w2v = sklearn.model_selection.GridSearchCV(gboosting, \n",
    "                                                            gboosting_parameters,\n",
    "                                                            scoring = \"accuracy\",\n",
    "                                                            cv = 5,\n",
    "                                                            n_jobs = -1\n",
    "                                                           )\n",
    "\n",
    "gboosting_classifier_w2v.fit(w2v_train_x, under_y)\n",
    "\n",
    "print(\"Word2vec vectorizer:\")\n",
    "print(f\"best accuracy of 'gradient boosting' model with word2vec is {gboosting_classifier_w2v.best_score_}\")\n",
    "print(f\"best parameters for 'gradient boosting' model with word2vec is{gboosting_classifier_w2v.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70c95",
   "metadata": {},
   "source": [
    "### Simple transformer\n",
    "\n",
    "Simple transformers get a *dataframe* for an input and not vectorized. Using the *train_test_split* function on the training set, I defined a new train and validation set to check the performance of our model. After setting the column names and passing the x and y for the dataframe, I checked for null values to see if everyhting is done right. \n",
    "\n",
    "The transformer arguments I defined are number of epochs and an option to over-writing the output on the directory used.\n",
    "\n",
    "In the final model, after trying out a few different parameters, I chose the *reberta* model and passed its name. Specified the number of the target classes, and I needed to specify that the cuda should not be used.\n",
    "\n",
    "Unfortunately, The model did not work on more than **35** data points!!! Due to this problem I had to eliminate this part of model training.\n",
    "\n",
    "I also had to eliminate the aprt that we needed to encode the target label since we do not use this model any more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420937c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3230ace",
   "metadata": {},
   "source": [
    "The best performance is for Gradient Boosting using TF-IDF vectorizer.\n",
    "\n",
    "Now we have to test it on our unseen dataset to evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28584ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 13438)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector = vectorizer.transform(test_x).toarray()\n",
    "test_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7099d8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = gboosting_classifier_tf.predict(test_vector)\n",
    "predicted_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4aaa288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(test_y, predicted_y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82461e7",
   "metadata": {},
   "source": [
    "The accuracy score is high for the test set too. More than 95% of the labels were predicted right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0559470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95849057, 0.94827586, 0.97667638, 0.92857143])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = sklearn.metrics.precision_score(test_y, predicted_y, average = None)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb052794",
   "metadata": {},
   "source": [
    "Precision score shows how much the perfirmane is good due to not labeling a negative label as positive. Which is high for htis modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1f4f1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96212121, 0.94827586, 0.95988539, 0.94463668])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = sklearn.metrics.recall_score(test_y, predicted_y, average = None)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6ec32",
   "metadata": {},
   "source": [
    "The higher recall score for each class indicates that the positive labels were predicted in a correct way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b651d46",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05c424",
   "metadata": {},
   "source": [
    "This pipeline is a supervised sentiment analysis on a sample dataset of 5000 data points. The original dataset was about a million data points containing different tweets in different languages. The modeling is done only on *English* language. \n",
    "\n",
    "To find the best classification model, I used two different vectorization methods and two different models.\n",
    "\n",
    "The models resulted in higher performance with TF-IDF vectorization. Since the word2vec vectorizer is basicaly used for words and not for texts, my function calculates the average of the text gievn, and it results in poor perfomance since the sentiment of words are lost and cannot be kept.\n",
    "\n",
    "This model is working with high accuracy, but in real business world it should be perfect. It could be monitored in a regular basis and improved. \n",
    "- We could improve the stopword corpus, we could deal with some noisy words that are not in the dictionary but express some imotions. For example \"looooove\". \n",
    "- We could define some emojy corpus wich can affect the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d28a97",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Analytics Vidhya website (2022) 'Beginners Guide to Regular Expressions in Natural Language Processing'. Available at: https://www.analyticsvidhya.com/blog/2021/03/beginners-guide-to-regular-expressions-in-natural-language-processing/ (Accessed: 01.12.2022)\n",
    "* Higging Face website (2022) 'Transformers'. Available at: https://huggingface.co/docs/transformers/index (Accessed: 20.11.2022)\n",
    "* imbalanced learn website (2022) 'RandomUnderSampler'. Available at: https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler (Accessed: 12.12.2022)\n",
    "* matplotlib (2022) 'Pyplot tutorial'. Available at: https://matplotlib.org/stable/tutorials/introductory/pyplot.html (Accessed: 20.11.2022)\n",
    "* MUHAMMAD TARIQ (kaggle website) (2022) 'Sentiment Dataset with 1 Million Tweets' Available at: https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets (Accessed: 02.11.2022)\n",
    "* Pandas website (2022) 'Pnadas User Guide'. Available at: https://pandas.pydata.org/docs/user_guide/index.html (Accessed: 11.12.2022)\n",
    "* scikit learn website (2022) 'sklearn.model_selection'. Available at: https://scikit-learn.org/stable/model_selection.html (Accessed: 17.11.2022)\n",
    "* Stack Over Flow website (2022) Available at: https://stackoverflow.com/ (Accessed: 16.12.2022)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
